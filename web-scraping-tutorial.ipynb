{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adebayoolalekan/web-scraping-tutorial?scriptVersionId=206158025\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### What is Web Scraping?\n#### The dictionary meaning of word ‘Scrapping’ implies getting something from the web. Here two questions arise: What we can get from the web and How to get that. The answer to the first question is ‘data’. Data is indispensable for any programmer and the basic requirement of every programming project is the large amount of useful data.\n\n#### The answer to the second question is a bit tricky, because there are lots of ways to get data. In general, we may get data from a database or data file and other sources. But what if we need large amount of data that is available online? One way to get such kind of data is to manually search (clicking away in a web browser) and save (copy-pasting into a spreadsheet or file) the required data. This method is quite tedious and time consuming. Another way to get such data is using web scraping.\n\n#### Web scraping, also called web data mining or web harvesting, is the process of constructing an agent which can extract, parse, download and organize useful information from the web automatically. In other words, we can say that instead of manually saving the data from websites, the web scraping software will automatically load and extract data from multiple websites as per our requirement.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Web Crawling v/s Web Scraping**\nThe terms Web Crawling and Scraping are often used interchangeably as the basic concept of them is to extract data. However, they are different from each other. We can understand the basic difference from their definitions.\n\nWeb crawling is basically used to index the information on the page using bots aka crawlers. It is also called indexing. On the hand, web scraping is an automated way of extracting the information using bots aka scrapers. It is also called data extraction.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Uses of Web Scraping**\nThe uses and reasons for using web scraping are as endless as the uses of the World Wide Web. Web scrapers can do anything like ordering online food, scanning online shopping website for you and buying ticket of a match the moment they are available etc. just like a human can do. Some of the important uses of web scraping are discussed here −\n\n* E-commerce Websites − Web scrapers can collect the data specially related to the price of a specific product from various e-commerce websites for their comparison.\n\n* Content Aggregators − Web scraping is used widely by content aggregators like news aggregators and job aggregators for providing updated data to their users.\n\n* Marketing and Sales Campaigns − Web scrapers can be used to get the data like emails, phone number etc. for sales and marketing campaigns.\n\n* Search Engine Optimization (SEO) − Web scraping is widely used by SEO tools like SEMRush, Majestic etc. to tell business how they rank for search keywords that matter to them.\n\n* Data for Machine Learning Projects − Retrieval of data for machine learning projects depends upon web scraping.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Components of a Web Scraper**\nA web scraper consists of the following components −\n\n* Web Crawler Module\nA very necessary component of web scraper, web crawler module, is used to navigate the target website by making HTTP or HTTPS request to the URLs. The crawler downloads the unstructured data (HTML contents) and passes it to extractor, the next module.\n\n* Extractor\nThe extractor processes the fetched HTML content and extracts the data into semistructured format. This is also called as a parser module and uses different parsing techniques like Regular expression, HTML Parsing, DOM parsing or Artificial Intelligence for its functioning.\n\n* Data Transformation and Cleaning Module\nThe data extracted above is not suitable for ready use. It must pass through some cleaning module so that we can use it. The methods like String manipulation or regular expression can be used for this purpose. Note that extraction and transformation can be performed in a single step also.\n\n* Storage Module\nAfter extracting the data, we need to store it as per our requirement. The storage module will output the data in a standard format that can be stored in a database or JSON or CSV format.","metadata":{}},{"cell_type":"markdown","source":"![](http://www.tutorialspoint.com/python_web_scraping/images/web_scraper.jpg)","metadata":{}},{"cell_type":"markdown","source":"Requests\nIt is a simple python web scraping library. It is an efficient HTTP library used for accessing web pages. With the help of Requests, we can get the raw HTML of web pages which can then be parsed for retrieving the data. Before using requests, let us understand its installation.","metadata":{}},{"cell_type":"markdown","source":"**Public APIs:**\n\n* Websites with open APIs provide structured data like JSON or XML.\n* Examples:\n* JSONPlaceholder: A free fake API for testing and prototyping.\n* OpenWeatherMap: Provides weather data.\n* CoinGecko API: Provides cryptocurrency data.","metadata":{}},{"cell_type":"code","source":"import requests\n\n# Example API URL\nurl = 'https://en.wikipedia.org/wiki/Special:Search?go=Go&search=world+richest&ns0=1'\n\n# Sending a GET request\nresponse = requests.get(url)\n\n# Print the JSON data\nprint(response.content)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:14.058758Z","iopub.execute_input":"2024-10-30T13:15:14.059167Z","iopub.status.idle":"2024-10-30T13:15:14.760623Z","shell.execute_reply.started":"2024-10-30T13:15:14.059129Z","shell.execute_reply":"2024-10-30T13:15:14.759397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Urllib3**\nIt is another Python library that can be used for retrieving data from URLs similar to the requests library. You can read more on this at its technical documentation at https://urllib3.readthedocs.io/en/latest/.","metadata":{}},{"cell_type":"code","source":"#Sending HTTP Requests\n#A basic HTTP GET request:\nurl = 'https://example.com'\n\n# Sending a GET request\nresponse = requests.get(url)\n\n# Check if the request was successful\nprint(response.status_code)  # 200 means successful\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:14.762876Z","iopub.execute_input":"2024-10-30T13:15:14.763249Z","iopub.status.idle":"2024-10-30T13:15:14.823797Z","shell.execute_reply.started":"2024-10-30T13:15:14.763211Z","shell.execute_reply":"2024-10-30T13:15:14.822549Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(response.content)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:14.825645Z","iopub.execute_input":"2024-10-30T13:15:14.826149Z","iopub.status.idle":"2024-10-30T13:15:14.832112Z","shell.execute_reply.started":"2024-10-30T13:15:14.826099Z","shell.execute_reply":"2024-10-30T13:15:14.830625Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Handling HTTP Responses**\nThe response object contains several important attributes:\n\n* response.text: Returns the HTML content of the page as a string.\n* response.content: Returns the raw bytes of the content (useful for non-text content like images).\n* response.status_code: The HTTP status code (e.g., 200 for success).\n* response.headers: HTTP headers of the response.","metadata":{}},{"cell_type":"code","source":"# Print the HTML content of the page\nprint(response.text)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:14.833821Z","iopub.execute_input":"2024-10-30T13:15:14.834272Z","iopub.status.idle":"2024-10-30T13:15:14.841923Z","shell.execute_reply.started":"2024-10-30T13:15:14.834224Z","shell.execute_reply":"2024-10-30T13:15:14.840508Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print HTTP headers\nprint(response.headers)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:14.845918Z","iopub.execute_input":"2024-10-30T13:15:14.846334Z","iopub.status.idle":"2024-10-30T13:15:14.853769Z","shell.execute_reply.started":"2024-10-30T13:15:14.846297Z","shell.execute_reply":"2024-10-30T13:15:14.852197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Introduction to BeautifulSoup**\nBeautifulSoup is used to parse and navigate HTML documents easily. \n\nIt allows us to extract meaningful data from the HTML page.\n\n**Parsing HTML**\nYou need to create a BeautifulSoup object from the HTML content returned by requests.","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\n# Parsing HTML content with BeautifulSoup\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Pretty printing the HTML\nprint(soup.prettify())\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:14.855511Z","iopub.execute_input":"2024-10-30T13:15:14.855993Z","iopub.status.idle":"2024-10-30T13:15:15.059637Z","shell.execute_reply.started":"2024-10-30T13:15:14.855945Z","shell.execute_reply":"2024-10-30T13:15:15.058257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Navigating the HTML Tree\n* You can navigate and search through the HTML document using various methods:\n\n* find(): Finds the first matching tag.\n* find_all(): Finds all matching tags.","metadata":{}},{"cell_type":"code","source":"# Finding the first <h1> tag\nh1_tag = soup.find('h1')\nprint(h1_tag.text)\n\n# Finding all <a> tags (links)\na_tags = soup.find_all('a')\nfor tag in a_tags:\n    print(tag.text, tag['href'])  # Print the link text and URL","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:15.061091Z","iopub.execute_input":"2024-10-30T13:15:15.061623Z","iopub.status.idle":"2024-10-30T13:15:15.068985Z","shell.execute_reply.started":"2024-10-30T13:15:15.061587Z","shell.execute_reply":"2024-10-30T13:15:15.067651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Practical Examples**\nLet's see how we can use requests and BeautifulSoup to extract specific data from a website.\n\n* Extracting Headings from a Page\nThis example extracts all the headings (\"h1\", \"h2\", etc.) from a page.","metadata":{}},{"cell_type":"code","source":"# Extract all headings (h1, h2, h3, etc.)\nfor i in range(1, 7):\n    headings = soup.find_all(f'h{i}')\n    for heading in headings:\n        print(f\"H{i}: {heading.text}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.070764Z","iopub.execute_input":"2024-10-30T13:15:15.071208Z","iopub.status.idle":"2024-10-30T13:15:15.079681Z","shell.execute_reply.started":"2024-10-30T13:15:15.071158Z","shell.execute_reply":"2024-10-30T13:15:15.078393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Extracting Links from a Web Page\n* This example extracts all the hyperlinks (<a> tags) from the page and prints their text and href attributes.","metadata":{}},{"cell_type":"code","source":"# Extract all links from the page\nlinks = soup.find_all('a')\nfor link in links:\n    href = link.get('href')\n    text = link.text\n    print(f\"Text: {text}, URL: {href}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.081279Z","iopub.execute_input":"2024-10-30T13:15:15.081795Z","iopub.status.idle":"2024-10-30T13:15:15.090529Z","shell.execute_reply.started":"2024-10-30T13:15:15.081744Z","shell.execute_reply":"2024-10-30T13:15:15.089222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.091886Z","iopub.execute_input":"2024-10-30T13:15:15.092374Z","iopub.status.idle":"2024-10-30T13:15:15.100626Z","shell.execute_reply.started":"2024-10-30T13:15:15.092314Z","shell.execute_reply":"2024-10-30T13:15:15.099399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url = \"https://www.scrapethissite.com/pages/forms/\"\npage = requests.get(url)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.102202Z","iopub.execute_input":"2024-10-30T13:15:15.102712Z","iopub.status.idle":"2024-10-30T13:15:15.29143Z","shell.execute_reply.started":"2024-10-30T13:15:15.102664Z","shell.execute_reply":"2024-10-30T13:15:15.290171Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup = BeautifulSoup(page.text, \"html\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.292912Z","iopub.execute_input":"2024-10-30T13:15:15.29339Z","iopub.status.idle":"2024-10-30T13:15:15.324265Z","shell.execute_reply.started":"2024-10-30T13:15:15.293317Z","shell.execute_reply":"2024-10-30T13:15:15.323151Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(soup.prettify)","metadata":{"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:15.325726Z","iopub.execute_input":"2024-10-30T13:15:15.326163Z","iopub.status.idle":"2024-10-30T13:15:15.345245Z","shell.execute_reply.started":"2024-10-30T13:15:15.326115Z","shell.execute_reply":"2024-10-30T13:15:15.343901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Find And Find_All","metadata":{}},{"cell_type":"code","source":"soup.find(\"div\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:15.351231Z","iopub.execute_input":"2024-10-30T13:15:15.351619Z","iopub.status.idle":"2024-10-30T13:15:15.362679Z","shell.execute_reply.started":"2024-10-30T13:15:15.351581Z","shell.execute_reply":"2024-10-30T13:15:15.361107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup.find_all(\"div\", class_=\"col-md-12\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:15.364578Z","iopub.execute_input":"2024-10-30T13:15:15.365374Z","iopub.status.idle":"2024-10-30T13:15:15.376482Z","shell.execute_reply.started":"2024-10-30T13:15:15.365301Z","shell.execute_reply":"2024-10-30T13:15:15.375261Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup.find_all(\"p\",class_ =\"lead\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.378061Z","iopub.execute_input":"2024-10-30T13:15:15.379195Z","iopub.status.idle":"2024-10-30T13:15:15.388685Z","shell.execute_reply.started":"2024-10-30T13:15:15.379145Z","shell.execute_reply":"2024-10-30T13:15:15.387539Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup.find_all(\"p\", class_ =\"lead\").text","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:15.390093Z","iopub.execute_input":"2024-10-30T13:15:15.391137Z","iopub.status.idle":"2024-10-30T13:15:16.044635Z","shell.execute_reply.started":"2024-10-30T13:15:15.391086Z","shell.execute_reply":"2024-10-30T13:15:16.043156Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup.find(\"p\", class_ =\"lead\").text.strip()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:37.328414Z","iopub.execute_input":"2024-10-30T13:15:37.328818Z","iopub.status.idle":"2024-10-30T13:15:37.3366Z","shell.execute_reply.started":"2024-10-30T13:15:37.328779Z","shell.execute_reply":"2024-10-30T13:15:37.335412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup.find_all(\"th\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:37.746621Z","iopub.execute_input":"2024-10-30T13:15:37.747288Z","iopub.status.idle":"2024-10-30T13:15:37.760143Z","shell.execute_reply.started":"2024-10-30T13:15:37.747233Z","shell.execute_reply":"2024-10-30T13:15:37.758802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soup.find(\"th\").text.strip()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:38.124331Z","iopub.execute_input":"2024-10-30T13:15:38.125168Z","iopub.status.idle":"2024-10-30T13:15:38.132176Z","shell.execute_reply.started":"2024-10-30T13:15:38.125118Z","shell.execute_reply":"2024-10-30T13:15:38.131021Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" table = soup.find('table', {'class': 'table'})","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:38.596389Z","iopub.execute_input":"2024-10-30T13:15:38.596898Z","iopub.status.idle":"2024-10-30T13:15:38.603094Z","shell.execute_reply.started":"2024-10-30T13:15:38.596849Z","shell.execute_reply":"2024-10-30T13:15:38.601703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"table","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T13:15:39.105407Z","iopub.execute_input":"2024-10-30T13:15:39.105856Z","iopub.status.idle":"2024-10-30T13:15:39.144201Z","shell.execute_reply.started":"2024-10-30T13:15:39.105813Z","shell.execute_reply":"2024-10-30T13:15:39.142889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Initialize lists to store data\ndata = []\n    \n    # Extract headers\nheaders = []\nfor th in table.find_all('th'):\n  headers.append(th.text.strip())\nheaders","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:39.635276Z","iopub.execute_input":"2024-10-30T13:15:39.63639Z","iopub.status.idle":"2024-10-30T13:15:40.062351Z","shell.execute_reply.started":"2024-10-30T13:15:39.636328Z","shell.execute_reply":"2024-10-30T13:15:40.061153Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract team data\nfor row in table.find_all('tr', {'class': 'team'}):\n    team_data = {}\n # Team name\n    team_data['Team Name'] = row.find('td', {'class': 'name'}).text.strip()\n        \n    team_data['Year'] = int(row.find('td', {'class': 'year'}).text.strip())\n        \n        # Wins\n    team_data['Wins'] = int(row.find('td', {'class': 'wins'}).text.strip())\n        \n        # Losses\n    team_data['Losses'] = int(row.find('td', {'class': 'losses'}).text.strip())\n        \n        # OT Losses (might be empty for some years)\n    ot_losses = row.find('td', {'class': 'ot-losses'}).text.strip()\n    team_data['OT Losses'] = int(ot_losses) if ot_losses else None\n        \n        # Win Percentage\n    team_data['Win %'] = float(row.find('td', {'class': 'pct'}).text.strip())\n        \n        # Goals For\n    team_data['Goals For'] = int(row.find('td', {'class': 'gf'}).text.strip())\n        \n        # Goals Against\n    team_data['Goals Against'] = int(row.find('td', {'class': 'ga'}).text.strip())\n        \n        # Goal Differential\n    team_data['+/-'] = int(row.find('td', {'class': 'diff'}).text.strip())\n        \n    data.append(team_data)\n    \n    # Create DataFrame\ndf = pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:40.305723Z","iopub.execute_input":"2024-10-30T13:15:40.306293Z","iopub.status.idle":"2024-10-30T13:15:40.337051Z","shell.execute_reply.started":"2024-10-30T13:15:40.306253Z","shell.execute_reply":"2024-10-30T13:15:40.335884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:47.350971Z","iopub.execute_input":"2024-10-30T13:15:47.351385Z","iopub.status.idle":"2024-10-30T13:15:47.366257Z","shell.execute_reply.started":"2024-10-30T13:15:47.351324Z","shell.execute_reply":"2024-10-30T13:15:47.365058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-30T13:15:47.368278Z","iopub.execute_input":"2024-10-30T13:15:47.36872Z","iopub.status.idle":"2024-10-30T13:15:47.398373Z","shell.execute_reply.started":"2024-10-30T13:15:47.368671Z","shell.execute_reply":"2024-10-30T13:15:47.397249Z"},"trusted":true},"outputs":[],"execution_count":null}]}